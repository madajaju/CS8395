= Scaling Strategies

== 1. Motivation: Growth Breaks Assumptions

Systems are often designed for early traffic, then suddenly face:

- A growing user base
- Seasonal spikes
- Unplanned load from a new customer or feature

When load increases, what used to be "fast enough" can become slow or unreliable. Scaling is about **sustaining performance and reliability as demand grows**.

---

== 2. Two Fundamental Directions: Vertical vs Horizontal

**Vertical scaling (scale up)**
Add more resources to a single machine:

- More CPU
- More memory
- Faster storage

**Horizontal scaling (scale out)**
Add more machines and distribute load:

- More servers behind a load balancer
- Partition data or services across nodes

Tradeoff summary:

- Vertical scaling is simpler but hits hardware limits.
- Horizontal scaling is more flexible but increases system complexity.

[plantuml]
....
@startuml
  title Scaling Directions
rectangle User
left to right direction
  rectangle "Vertical Scaling" as VS {
    User --> Hardware : Add CPU, Memory
  }
  rectangle "Horizontal Scaling" as HS {
    User -> LoadBalancer : Distribute Requests
    User --> Servers : Add More Machines
  }
@enduml
....

---

== 3. Stateless vs Stateful Implications

Scaling strategies depend on whether the service is **stateless** or **stateful**.

- **Stateless services** scale horizontally easily:
- Any node can handle any request.
- Use a load balancer to distribute traffic.

- **Stateful services** are harder:
- Session data or local state ties requests to a specific node.
- Requires sticky sessions, replication, or external state stores.

This is why many architectures push state **out** of application servers and into shared systems.

[plantuml]
....
@startuml
  title Stateless vs Stateful
left to right direction
  rectangle "Stateless Services" as Stateless {
    Stateless -> LoadBalancer : Handle Any Request
  }
  rectangle "Stateful Services" as Stateful {
    Stateful -> StickySessions : Maintain Session
    Stateful -> Replication : Share State
  }
@enduml
....

---

== 4. The Scaling Ladder: Common Tactics

Teams usually scale in stages:

1. **Optimize the code**
Reduce expensive calls, improve queries, add indexes.

2. **Cache aggressively**
Keep repeated results in memory (Redis, CDN, in-process cache).

3. **Scale vertically**
Buy bigger machines for immediate relief.

4. **Scale horizontally**
Add more nodes and distribute load.

5. **Partition data**
Shard databases or split services.

Each step adds cost and complexity; teams should not jump to the most complex step first.

---

== 5. Load Balancing and Traffic Distribution

Horizontal scaling usually depends on a **load balancer**:

- Routes requests across multiple servers
- Health checks remove failing nodes
- Can be layer 4 (TCP) or layer 7 (HTTP)

Key concerns:

- **Even distribution** of load
- **Fast failover**
- **Minimal overhead**

[plantuml]
....
@startuml
  title Load Balancing
  actor User
  node "Load Balancer" {
    User -> LoadBalancer : Send Request
    LoadBalancer --> ServerA : "Distribute Load"
    LoadBalancer --> ServerB : "Distribute Load"
  }
@enduml
....

---

== 6. Caching as a Scaling Strategy

Caching reduces load without adding servers:

- **Client-side caching** (browser cache, mobile cache)
- **Edge caching** (CDNs)
- **Server-side caching** (Redis, in-memory caches)

Caching tradeoffs:

- Stale data
- Cache invalidation complexity
- Memory costs

Good caching strategy often delays the need for more expensive scaling steps.

---

== 7. Data Layer Scaling

The data layer is often the bottleneck. Common approaches:

- **Read replicas**: copy data for read-heavy workloads
- **Sharding**: split data across multiple databases
- **Partitioning**: split data by key or time

Tradeoffs:

- Replication lag
- Complex queries across shards
- Operational overhead for backups and migrations

Scaling data is usually the hardest part of scaling systems.

[plantuml]
....
@startuml
  title Data Layer Scaling
  rectangle "Read Replicas" as RR
  rectangle "Sharding" as SH
  rectangle "Partitioning" as P
    database Database
  P --> Database : "Split by Key/Time"
  RR --> Database : "Support Reads"
  SH --> Database : "Split Data"
@enduml
....

---

== 8. Asynchronous Workloads and Queues

Not all scaling is about handling live requests. Offload work:

- Use **message queues** (SQS, RabbitMQ, Kafka)
- Process tasks asynchronously

Benefits:

- Smooths out spikes
- Keeps user-facing latency low
- Allows background workers to scale separately

This is a core scaling tool for workloads like video processing, email sending, and analytics pipelines.

---

== 9. Cost and Complexity Tradeoffs

Scaling is not free:

- More servers mean more cost
- More moving parts mean more failure modes
- Operational complexity grows with size

Teams must balance **performance goals** with **cost constraints**.

Sometimes the best scaling move is:

- Reducing features that drive expensive load
- Improving efficiency instead of adding machines

---

== 10. Example Scenario

Imagine a product that launches and traffic doubles:

1. The team adds caching to reduce database hits.
2. They scale the web tier horizontally behind a load balancer.
3. The database becomes the new bottleneck.
4. They add read replicas and later shard by customer ID.

Each step is more complex but also more scalable.

---

== 11. Key Takeaways

- Scaling strategies range from simple optimizations to complex distributed systems.
- Vertical scaling is easy but limited; horizontal scaling is powerful but complex.
- Stateless services scale more easily than stateful ones.
- Caching and asynchronous processing are often the fastest scaling wins.
- The data layer is usually the hardest part to scale.