1
00:00:00,100 --> 00:00:00,500
All right.

2
00:00:00,500 --> 00:00:04,333
In this lecture I'm going to talk about
scaling strategies for microservices.

3
00:00:04,700 --> 00:00:09,066
So this is just the beginning
of of scaling.

4
00:00:09,800 --> 00:00:11,333
We're going to go more into depth.

5
00:00:11,333 --> 00:00:14,800
There's several several different lectures
on different scaling strategies.

6
00:00:15,266 --> 00:00:19,666
But this is kind of the,
the top high level overview

7
00:00:19,666 --> 00:00:23,566
of some of those scaling strategies
that you should be looking at, right.

8
00:00:23,833 --> 00:00:28,833
Systems are often designed early
for just traffic, minimal traffic.

9
00:00:28,833 --> 00:00:31,933
And then they sometimes will experience,

10
00:00:31,933 --> 00:00:35,466
a growing user base or seasonal spikes
or whatever the case may be.

11
00:00:35,966 --> 00:00:40,666
I released
a, a new product when I was CIO,

12
00:00:41,166 --> 00:00:44,233
and we released a new product,
and we did not scale appropriately,

13
00:00:44,233 --> 00:00:47,133
and we were down within,
like 30 minutes of launching.

14
00:00:47,133 --> 00:00:50,033
He was a total disaster
because we did not plan

15
00:00:50,033 --> 00:00:53,333
for the scalability that, that hit us.

16
00:00:53,566 --> 00:00:57,266
We weren't even close to
to getting, the numbers that that hit us.

17
00:00:57,666 --> 00:01:00,533
The idea here
is that when a load does increase,

18
00:01:00,533 --> 00:01:05,166
that we can act fast enough
so that we don't lose users

19
00:01:05,166 --> 00:01:10,866
and so that we don't lose, 
other, applications

20
00:01:10,866 --> 00:01:14,966
that are using our stuff
through, timeouts and through,

21
00:01:15,933 --> 00:01:18,266
retries that aren't, are being serviced.

22
00:01:18,266 --> 00:01:23,233
So we need to be able
to sustain this growth and not just throw,

23
00:01:23,233 --> 00:01:25,433
hey, I'm just going to throw
a whole bunch of machines at it.

24
00:01:25,433 --> 00:01:28,166
But we also need to shrink as well.

25
00:01:28,166 --> 00:01:28,433
Okay.

26
00:01:28,433 --> 00:01:31,166
So there's two fundamental directions
in scaling.

27
00:01:31,166 --> 00:01:33,400
There's horizontal and vertical.

28
00:01:33,400 --> 00:01:37,233
So with vertical I'm
adding resources to a single machine.

29
00:01:37,233 --> 00:01:40,500
More CPUs more memory faster storage.

30
00:01:40,500 --> 00:01:43,066
That's vertical scaling right.

31
00:01:43,066 --> 00:01:47,333
This is expensive to do,
with resources and time.

32
00:01:47,333 --> 00:01:51,166
If I already have it on one machine
and I need to scale vertically,

33
00:01:51,166 --> 00:01:54,700
how do I add more CPUs, more memory
and storage?

34
00:01:55,066 --> 00:01:56,933
That can be very difficult to do.

35
00:01:56,933 --> 00:01:59,266
Let's. I'm using some cloud techniques.

36
00:01:59,266 --> 00:02:02,200
I can scale vertically and dynamically,

37
00:02:02,200 --> 00:02:05,400
but it is it is, more difficult.

38
00:02:06,100 --> 00:02:08,533
A better way to go that gives me,

39
00:02:09,566 --> 00:02:11,066
more machines

40
00:02:11,066 --> 00:02:15,366
and a more distributed load, but adds
maybe some latency to it

41
00:02:15,866 --> 00:02:18,866
is horizontal scaling
and this is scaling out.

42
00:02:18,866 --> 00:02:22,366
I can have more servers
behind a load balancer,

43
00:02:22,366 --> 00:02:26,066
but I am introducing more latency
to the system and more complexity.

44
00:02:26,633 --> 00:02:29,433
Right. And additional costs. Right.

45
00:02:29,433 --> 00:02:32,566
I can partition the data and services
across multiple nodes.

46
00:02:32,566 --> 00:02:35,566
These are all really good strategies.

47
00:02:36,433 --> 00:02:38,366
So if a user has

48
00:02:38,366 --> 00:02:42,500
a distributed
request comes in actually to your, your,

49
00:02:42,500 --> 00:02:46,700
microservice, orchestrator,
I need to distribute requests,

50
00:02:47,166 --> 00:02:50,800
a load balancer gets hit and then

51
00:02:50,800 --> 00:02:54,433
the servers are spun up to handle the,

52
00:02:54,466 --> 00:02:58,833
those new, requests
coming in with vertical scaling.

53
00:02:59,200 --> 00:03:01,066
I just add more CPUs and memory.

54
00:03:01,066 --> 00:03:04,066
Now, there are some really cool techniques
in virtualization

55
00:03:04,300 --> 00:03:07,766
that let me scale VMs up, but not down.

56
00:03:08,233 --> 00:03:09,333
So keep that in mind.

57
00:03:09,333 --> 00:03:12,666
Has a way of doing
virtual, vertical scaling.

58
00:03:14,666 --> 00:03:15,766
All right.

59
00:03:15,766 --> 00:03:19,100
Let's talk about stateless
and stateful implications of scaling.

60
00:03:19,100 --> 00:03:23,066
We've talked about stateless
and state for a microservices before.

61
00:03:23,066 --> 00:03:28,766
Keep this in mind that stateless
microservices are easier to scale

62
00:03:28,766 --> 00:03:31,766
because they have no state,

63
00:03:32,166 --> 00:03:34,833
which means they're not storing anything
locally.

64
00:03:34,833 --> 00:03:36,466
There's nothing persistent.

65
00:03:36,466 --> 00:03:38,266
And I can just

66
00:03:38,266 --> 00:03:41,333
click those things on very quickly
and spin them up quickly.

67
00:03:41,566 --> 00:03:44,500
So wherever you can in your application
where you think

68
00:03:44,500 --> 00:03:48,333
there's going to be a bottleneck
or you need to scale, you should work on

69
00:03:48,333 --> 00:03:52,333
making that a stateless microservice.

70
00:03:52,333 --> 00:03:55,333
If you can keep the stateful services,

71
00:03:55,600 --> 00:03:59,366
in another place, stateful
services can be scaled, but it's harder.

72
00:04:00,333 --> 00:04:03,700
You do have to store session
data in local,

73
00:04:04,166 --> 00:04:08,533
and so you've got to share some data
between some of the microservices

74
00:04:08,866 --> 00:04:13,633
tends to cause some coupling
that could, have problems.

75
00:04:13,633 --> 00:04:17,800
You have sticky sessions,
which is a new technique that you can have

76
00:04:17,800 --> 00:04:20,800
a session span, multiple microservices.

77
00:04:21,133 --> 00:04:24,133
So these are some techniques
that you can use.

78
00:04:24,133 --> 00:04:27,066
But like I said, if you can go stateless

79
00:04:27,066 --> 00:04:30,100
for scaling
that's a far better than stateful.

80
00:04:30,100 --> 00:04:32,700
But there are some techniques to
do stateful service scaling.

81
00:04:34,933 --> 00:04:35,266
Okay.

82
00:04:35,266 --> 00:04:37,833
Some common tactics.

83
00:04:37,833 --> 00:04:40,366
So you want to optimize your code
to reduce

84
00:04:40,366 --> 00:04:45,133
expensive calls and improve queries
and add indices and things like that.

85
00:04:45,133 --> 00:04:49,900
When you're talking about data,
you can cache things pretty aggressively.

86
00:04:49,900 --> 00:04:53,300
So you're not maybe going
and hitting another microservice.

87
00:04:53,300 --> 00:04:55,400
Every time you need something,
you can cache something

88
00:04:55,400 --> 00:04:58,866
that turns something into, more stateful.

89
00:04:59,533 --> 00:05:01,166
But there could be some benefit.

90
00:05:01,166 --> 00:05:04,466
And there are some in process caches

91
00:05:04,466 --> 00:05:07,933
like radius and CDN that you can use for,

92
00:05:07,933 --> 00:05:10,933
keeping these repeated results in memory
and reusing them.

93
00:05:11,700 --> 00:05:14,266
You can scale vertically.

94
00:05:14,266 --> 00:05:16,933
This is you by a bigger machine.

95
00:05:16,933 --> 00:05:21,300
Immediate release happens that way
for, new, new CPUs.

96
00:05:21,300 --> 00:05:22,866
I there's so many,

97
00:05:22,866 --> 00:05:26,833
cores in these brand new servers
that are out there, it's amazing.

98
00:05:27,166 --> 00:05:28,233
And memory.

99
00:05:28,233 --> 00:05:31,900
And so you could go bigger vertically,
but that can

100
00:05:31,900 --> 00:05:35,466
cause cause
you some, expense that you don't want.

101
00:05:36,733 --> 00:05:37,366
Right?

102
00:05:37,366 --> 00:05:42,666
You can look at horizontal scaling
by adding more nodes in and distributing

103
00:05:42,666 --> 00:05:43,066
load.

104
00:05:43,066 --> 00:05:46,433
You have to make sure if you're
architected properly, this is pretty easy

105
00:05:46,833 --> 00:05:50,000
on, on stateless,

106
00:05:50,000 --> 00:05:53,733
microservices,
if you're having a data problem

107
00:05:53,733 --> 00:05:58,600
and you want to scale horizontally,
then you come up with, data partitioning,

108
00:05:59,133 --> 00:06:02,733
strategies like sharding databases
or splitting services.

109
00:06:03,600 --> 00:06:07,100
And making sure we kind of do our best
to keep that

110
00:06:07,100 --> 00:06:10,800
a microservice has its own database,
concept in there.

111
00:06:13,200 --> 00:06:13,600
Okay.

112
00:06:13,600 --> 00:06:14,666
Load balancing.

113
00:06:14,666 --> 00:06:17,166
So load
balancing is pretty straightforward.

114
00:06:17,166 --> 00:06:22,166
I've got a someone is sending a request
in, it hits a load balancer first,

115
00:06:22,800 --> 00:06:27,466
then the load balancer picks
which server to, send the request

116
00:06:27,466 --> 00:06:32,433
to to distribute the, the, 
the load the requests,

117
00:06:32,900 --> 00:06:36,500
and then that that server
actually responds back to the user.

118
00:06:37,700 --> 00:06:38,233
Right.

119
00:06:38,233 --> 00:06:41,033
There's lots of different, 
load balancers out there.

120
00:06:41,033 --> 00:06:44,733
Some are at the network level
and some are at the application level.

121
00:06:45,200 --> 00:06:47,833
So either a TCP or up at the application

122
00:06:47,833 --> 00:06:50,833
layer at layer seven http

123
00:06:51,033 --> 00:06:54,433
health checks can help check if server

124
00:06:54,433 --> 00:06:59,533
A is, running slow compared to server
B, things like that.

125
00:06:59,866 --> 00:07:04,033
Lots of different strategies on these load
balancers and schedulers, right?

126
00:07:04,466 --> 00:07:08,866
Typically round robin works pretty good
where I just go around in a circle, right?

127
00:07:08,933 --> 00:07:11,933
That tends to work pretty well
for most systems.

128
00:07:12,000 --> 00:07:15,633
More complex
and more busy systems have a much more,

129
00:07:15,666 --> 00:07:18,633
sophisticated,

130
00:07:18,633 --> 00:07:21,633
load balancing, algorithms.

131
00:07:22,666 --> 00:07:23,166
Okay.

132
00:07:23,166 --> 00:07:26,766
Some of the concerns you want, though,
is that, evenly distributing the load

133
00:07:26,966 --> 00:07:31,333
that have fast failover and with minimal
overhead as much as possible.

134
00:07:31,333 --> 00:07:34,200
So you want to make sure
that schedulers really fast.

135
00:07:35,900 --> 00:07:40,233
Some other things to help with, 
scaling is

136
00:07:40,500 --> 00:07:43,833
if I see that
I'm let's say that I have a,

137
00:07:45,166 --> 00:07:48,400
a zip code, a zip code lookup service.

138
00:07:48,733 --> 00:07:50,066
Okay. Very cool.

139
00:07:50,066 --> 00:07:51,766
Right. A zip code lookup service.

140
00:07:51,766 --> 00:07:55,433
You give me an address
and I look up the zip code for you.

141
00:07:55,666 --> 00:07:56,900
Very cool.

142
00:07:56,900 --> 00:08:02,166
Let's say that,
I've got a, a shipping server.

143
00:08:02,866 --> 00:08:03,166
Right.

144
00:08:03,166 --> 00:08:08,200
And the shipping server
or service is geared towards a specific.

145
00:08:08,566 --> 00:08:12,433
I decided to create shipping services
based off of state.

146
00:08:12,700 --> 00:08:14,566
Which state are you in?

147
00:08:14,566 --> 00:08:17,933
I might want to do a little crazy thing
in cache.

148
00:08:17,933 --> 00:08:22,366
Some of those, zip codes
that I'm getting from the zip code service

149
00:08:22,833 --> 00:08:25,833
I based off of, address.

150
00:08:26,966 --> 00:08:29,100
And I may want to, cache that.

151
00:08:29,100 --> 00:08:32,300
So if someone's using the same address
multiple times

152
00:08:32,300 --> 00:08:35,633
that, I might be hitting that backend
server every time.

153
00:08:35,633 --> 00:08:39,200
This is a scaling strategy
that works pretty well as client side.

154
00:08:39,200 --> 00:08:41,566
Caching.

155
00:08:41,566 --> 00:08:44,600
There's server side caching as well.

156
00:08:45,200 --> 00:08:48,200
There's lots of different strategies here
for caching.

157
00:08:48,400 --> 00:08:52,000
The downside that you get with caching is
you can get stale data,

158
00:08:52,400 --> 00:08:55,500
you can get invalid, caches.

159
00:08:55,500 --> 00:08:59,866
So you got some complexity there and
you could increase your memory, usage.

160
00:09:00,033 --> 00:09:03,433
So it's an interesting
it's a good strategy,

161
00:09:03,433 --> 00:09:06,833
but she can't just store everything
and keep it forever.

162
00:09:06,833 --> 00:09:08,233
That can cause you some problems.

163
00:09:10,266 --> 00:09:10,466
All right.

164
00:09:10,466 --> 00:09:13,466
Let's talk about data layer scaling.

165
00:09:13,466 --> 00:09:16,033
If everything's
hitting the same database,

166
00:09:16,033 --> 00:09:20,300
I can absolutely run into some issues
here, right?

167
00:09:20,633 --> 00:09:22,533
Especially read and write.

168
00:09:22,533 --> 00:09:25,066
We all know that read is much faster
than.

169
00:09:25,066 --> 00:09:25,966
Right.

170
00:09:25,966 --> 00:09:28,966
So, I can have a bunch of read replicas,

171
00:09:29,366 --> 00:09:32,566
that I have,
spread throughout my architecture.

172
00:09:32,566 --> 00:09:34,100
That could be really interesting.

173
00:09:34,100 --> 00:09:37,366
Sharding is a technique
where I actually split the database

174
00:09:37,866 --> 00:09:40,866
up based off of some key in a table.

175
00:09:41,100 --> 00:09:45,466
And, 
so now, when someone asks for something,

176
00:09:45,466 --> 00:09:48,500
I can go to a specific shard
and get that database

177
00:09:48,500 --> 00:09:52,466
that has all the information
in that shard, based off that key,

178
00:09:53,900 --> 00:09:55,533
there are trade offs with this, and

179
00:09:55,533 --> 00:09:58,800
there are some really good back
end systems that handle this stuff

180
00:09:59,066 --> 00:10:03,133
pretty much seamlessly for you now,
which is really cool.

181
00:10:03,133 --> 00:10:07,500
Some great storage, techniques out there
that will do a lot of this stuff for you,

182
00:10:07,966 --> 00:10:11,733
but you still need to know that
the technique exists and that it is valid.

183
00:10:12,000 --> 00:10:13,900
Especially Kafka is a great example.

184
00:10:13,900 --> 00:10:17,100
They have partitioning and sharding
that you could do on your Kafka servers

185
00:10:17,566 --> 00:10:21,133
if you overwhelm them,
that, increase performance dramatically.

186
00:10:23,333 --> 00:10:25,466
Let's
talk about asynchronous workloads, right?

187
00:10:25,466 --> 00:10:28,766
This is where I just fire
and forget, right?

188
00:10:29,333 --> 00:10:31,600
Which is really kind of cool, right?

189
00:10:31,600 --> 00:10:35,466
Because I can just fire something off
and it's like a one way call,

190
00:10:35,700 --> 00:10:38,566
and I'm not waiting for,
something to come back.

191
00:10:38,566 --> 00:10:39,900
Typically you're going to do this

192
00:10:39,900 --> 00:10:43,466
with, like, message queues,
but you can make, process.

193
00:10:43,833 --> 00:10:49,166
You can do, asynchronous calls as well,
with other techniques.

194
00:10:49,466 --> 00:10:50,433
Those aren't the only ones.

195
00:10:50,433 --> 00:10:53,433
Just message cuz
you can go directly to sockets.

196
00:10:53,533 --> 00:10:56,933
And you don't need a response back
with the rest API.

197
00:10:57,300 --> 00:10:58,233
Not so much.

198
00:10:58,233 --> 00:11:00,700
It's always synchronous, right?

199
00:11:00,700 --> 00:11:04,166
Some of the benefits of this is it
smooths out any spikes,

200
00:11:04,600 --> 00:11:07,600
decreases the amount of traffic
that you have going on,

201
00:11:07,633 --> 00:11:10,600
and it keeps the user facing lower
latency.

202
00:11:10,600 --> 00:11:13,900
They get they
they get a response back quickly.

203
00:11:13,900 --> 00:11:17,400
They may not get an answer back
from back in service, but at least

204
00:11:17,833 --> 00:11:21,333
the user is not waiting
for something on the back.

205
00:11:21,333 --> 00:11:25,200
Yeah,
and it also allows the back end services

206
00:11:25,600 --> 00:11:28,600
and workers
to scale separately from the front end.

207
00:11:28,666 --> 00:11:31,966
And we will talk a lot about asynchronous
workloads and queues.

208
00:11:32,300 --> 00:11:33,600
I'll later on in the course

209
00:11:34,766 --> 00:11:36,166
write some of the trade offs.

210
00:11:36,166 --> 00:11:37,466
Scaling is not free.

211
00:11:37,466 --> 00:11:38,933
It's going to cost you money.

212
00:11:38,933 --> 00:11:41,866
Whether you're
increasing the amount of memory,

213
00:11:41,866 --> 00:11:45,266
whether you're adding more servers,
there's a lot more moving parts

214
00:11:45,266 --> 00:11:48,266
that increases, 
failure modes and complexity,

215
00:11:48,733 --> 00:11:52,233
grows,
which I've got to manage that as well.

216
00:11:52,700 --> 00:11:56,500
And you've got to balance this
performance calls versus cost constraints.

217
00:11:56,600 --> 00:11:58,633
Yeah. This is what makes you an architect.

218
00:11:58,633 --> 00:12:02,166
You've got to understand
what the cost benefits are of everything.

219
00:12:04,200 --> 00:12:04,666
Okay.

220
00:12:04,666 --> 00:12:06,000
An example scenario here.

221
00:12:06,000 --> 00:12:09,300
Imagine a product that launches
and the traffic doubles.

222
00:12:09,833 --> 00:12:10,066
All right.

223
00:12:10,066 --> 00:12:13,066
What strategies could I use?

224
00:12:13,266 --> 00:12:16,866
Right away
I could have caching your database hits.

225
00:12:17,600 --> 00:12:20,700
I can use, I could scale the web tier

226
00:12:20,700 --> 00:12:23,700
to horizontally behind a load balancer.

227
00:12:23,866 --> 00:12:28,266
I can, if the bottleneck
becomes a database, I can use sharding.

228
00:12:28,566 --> 00:12:31,666
And, read
replicas is a good way to go with that.

229
00:12:32,433 --> 00:12:35,433
There's lots of different steps
you can do on this.

230
00:12:35,433 --> 00:12:36,866
So we are going to talk

231
00:12:36,866 --> 00:12:40,066
a lot about different scaling techniques, 
this whole semester.

232
00:12:40,666 --> 00:12:43,433
So key takeaways is understand the pluses

233
00:12:43,433 --> 00:12:46,433
and minuses of the different scaling
techniques that you have.

234
00:12:46,733 --> 00:12:50,266
And look at and make sure
that you can understand vertical scaling

235
00:12:50,700 --> 00:12:53,700
and the benefits
of that over horizontal scaling.

236
00:12:54,033 --> 00:12:56,600
And then stateless
services are a whole lot easier

237
00:12:56,600 --> 00:12:59,600
to scale than stateful services.

238
00:12:59,666 --> 00:13:02,866
How you can use asynchronous
and caching techniques.

239
00:13:03,400 --> 00:13:06,400
That will give you, 
scaling pretty quick.

240
00:13:06,900 --> 00:13:10,733
And then also typically your data layer
usually is the hardest part to scale.

241
00:13:11,200 --> 00:13:13,800
So, watch out for that, where you can.
